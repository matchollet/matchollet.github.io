<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Mathieu Chollet">

  
  
  
    
  
  <meta name="description" content="IEEE In many applications, Embodied Conversational Agents (ECAs) must be able to express various affects such as emotions or social attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. Social attitudes affect the whole behavior of a person: they are &amp;#x201C;characteristic of an affective style that colors the entire interaction&amp;#x201D; (Scherer, 2005). Moreover, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with surrounding signals. Non-verbal behavior planning models designed to allow ECAs to express attitudes should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The contribution of this paper is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of a social phenomenon from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. This methodology is applied to design a virtual recruiter capable of expressing social attitudes, which is then evaluated in and out of an interaction context.">

  
  <link rel="alternate" hreflang="en-us" href="https://matchollet.github.io/publication/taffc17methodology/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://matchollet.github.io/publication/taffc17methodology/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Mathieu Chollet">
  <meta property="og:url" content="https://matchollet.github.io/publication/taffc17methodology/">
  <meta property="og:title" content="A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes | Mathieu Chollet">
  <meta property="og:description" content="IEEE In many applications, Embodied Conversational Agents (ECAs) must be able to express various affects such as emotions or social attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. Social attitudes affect the whole behavior of a person: they are &amp;#x201C;characteristic of an affective style that colors the entire interaction&amp;#x201D; (Scherer, 2005). Moreover, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with surrounding signals. Non-verbal behavior planning models designed to allow ECAs to express attitudes should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The contribution of this paper is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of a social phenomenon from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. This methodology is applied to design a virtual recruiter capable of expressing social attitudes, which is then evaluated in and out of an interaction context."><meta property="og:image" content="https://matchollet.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://matchollet.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2017-09-01T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2017-09-01T00:00:00&#43;00:00">
  

  


    











<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://matchollet.github.io/publication/taffc17methodology/"
  },
  "headline": "A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes",
  
  "datePublished": "2017-09-01T00:00:00Z",
  "dateModified": "2017-09-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "M. Chollet"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Mathieu Chollet",
    "logo": {
      "@type": "ImageObject",
      "url": "https://matchollet.github.io/img/icon-512.png"
    }
  },
  "description": "IEEE In many applications, Embodied Conversational Agents (ECAs) must be able to express various affects such as emotions or social attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. Social attitudes affect the whole behavior of a person: they are \u0026#x201C;characteristic of an affective style that colors the entire interaction\u0026#x201D; (Scherer, 2005). Moreover, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with surrounding signals. Non-verbal behavior planning models designed to allow ECAs to express attitudes should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The contribution of this paper is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of a social phenomenon from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. This methodology is applied to design a virtual recruiter capable of expressing social attitudes, which is then evaluated in and out of an interaction context."
}
</script>

  

  


  


  





  <title>A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes | Mathieu Chollet</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Mathieu Chollet</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <div class="pub">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/m.-chollet/">M. Chollet</a></span>, <span><a href="/authors/m.-ochs/">M. Ochs</a></span>, <span><a href="/authors/c.-pelachaud/">C. Pelachaud</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2017
  </span>
  

  

  

  
  
  

  
  

</div>

    













<div class="btn-links mb-3">
  
  








  









  
  <a class="btn btn-outline-primary my-1 mr-1" href="/project/tardis/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.1109/TAFFC.2017.2753777" target="_blank" rel="noopener">
  DOI
</a>



</div>


  
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">IEEE In many applications, Embodied Conversational Agents (ECAs) must be able to express various affects such as emotions or social attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. Social attitudes affect the whole behavior of a person: they are “characteristic of an affective style that colors the entire interaction” (Scherer, 2005). Moreover, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with surrounding signals. Non-verbal behavior planning models designed to allow ECAs to express attitudes should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The contribution of this paper is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of a social phenomenon from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. This methodology is applied to design a virtual recruiter capable of expressing social attitudes, which is then evaluated in and out of an interaction context.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            
            
            <a href="/publication/#2">
              Journal article
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><em>IEEE Transactions on Affective Computing</em></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    



<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/tardis/">Tardis</a>
  
  <a class="badge badge-light" href="/tags/virtual-agents/">Virtual Agents</a>
  
  <a class="badge badge-light" href="/tags/social-attitudes/">Social Attitudes</a>
  
</div>














  
  
    
  
  






  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/m.-chollet/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/iva15adaptative/">Towards a socially adaptive virtual agent</a></li>
      
      <li><a href="/publication/idgei15architecture/">An architecture for a socially adaptive virtual recruiter in job interview simulations</a></li>
      
      <li><a href="/publication/iva14sequence/">From non-verbal signals sequence mining to bayesian networks for interpersonal attitudes expression</a></li>
      
      <li><a href="/publication/wacai14expressing/">Expressing social attitudes in virtual agents for social coaching</a></li>
      
      <li><a href="/publication/aamas14expressing/">Expressing social attitudes in virtual agents for social coaching</a></li>
      
    </ul>
  </div>
  



  </div>
</div>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.8c16f756486dbe4bda816a966f48730d.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
