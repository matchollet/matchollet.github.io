@article{Chollet2017,
 abstract = {In many applications, Embodied Conversational Agents (ECAs) must be able to express various affects such as emotions or social attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. Social attitudes affect the whole behavior of a person: they are &#x201C;characteristic of an affective style that colors the entire interaction&#x201D; (Scherer, 2005). Moreover, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with surrounding signals. Non-verbal behavior planning models designed to allow ECAs to express attitudes should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The contribution of this paper is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of a social phenomenon from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. This methodology is applied to design a virtual recruiter capable of expressing social attitudes, which is then evaluated in and out of an interaction context.},
 author = {Chollet, M. and Ochs, M. and Pelachaud, C.},
 doi = {10.1109/TAFFC.2017.2753777},
 issn = {19493045},
 journal = {IEEE Transactions on Affective Computing},
 keywords = {Avatars,Color,Computational modeling,Embodied Conversational Agents,Interviews,Planning,Training,Videos,non-verbal signals sequences,social attitudes},
 title = {A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes},
 year = {2017}
}
