[{"authors":null,"categories":null,"content":"Hello there! I am an assistant professor at IMT Atlantique in Nantes, France. My research interests include intelligent virtual agents, multimodal machine learning applied to understanding human behavior, and interactive social skills training applications.\n","date":1546300800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1546300800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://matchollet.github.io/authors/admin/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hello!","tags":null,"title":"Mathieu Chollet","type":"widget_page"},{"authors":null,"categories":null,"content":"In the CICERO project, our aim is to investigate if public speaking skills can be improved using virtual training. Descriptions of the user\u0026rsquo;s public speaking behavior are automatically extracted from audiovisual sensors, and an interactive virtual audience is used to provide feedback to the users depending on their public speaking performance. To investigate how virtual characters can help improving the learning outcome of a public speaking training system, I proposed a flexible and modular architecture for interactive virtual audiences, and conducted a series of studies on the impact of different feedback strategies on training outcomes and user experiences.\n  ","date":1548284400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548284400,"objectID":"a80c5435cfcd812b914139a18faeb0b0","permalink":"https://matchollet.github.io/project/cicero/","publishdate":"2019-01-24T00:00:00+01:00","relpermalink":"/project/cicero/","section":"project","summary":"Public Speaking Training with Interactive Virtual Audiences","tags":["Cicero","Virtual Agents","Virtual Audiences","Multimodal Behavior Understanding","Social Skills Training"],"title":"Cicero","type":"project"},{"authors":null,"categories":null,"content":"I was involved in the EU FP7 Tardis Project. TARDIS aimed to build a scenario-based serious-game simulation platform for young people (ages 18-25) at risk of exclusion to explore, practice and improve their social skills. TARDIS facilitates the interaction through virtual agents (VAs) acting as recruiters in job interviews scenarios. My contribution in this project focused on the virtual recruiter. I proposed a data-driven model allowing VAs to express different interpersonal attitudes (dominant, friendly\u0026hellip;) which takes into account how the recruiter’s non-verbal behaviors are sequenced. The model uses a dataset of non-verbal behavior sequences obtained using sequence mining techniques, and can generate new sequences to express a chosen interpersonal attitude.\n  ","date":1548284400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548284400,"objectID":"af8b8ab25957c63f6bf8e9e63dab5763","permalink":"https://matchollet.github.io/project/tardis/","publishdate":"2019-01-24T00:00:00+01:00","relpermalink":"/project/tardis/","section":"project","summary":"Rehearsing for Job Interviews with Virtual Recruiters","tags":["Tardis","Virtual Agents","Social Skills Training","Social Attitudes"],"title":"Tardis","type":"project"},{"authors":null,"categories":null,"content":"With colleagues from the MedVR USC-ICT group, I was involved in the Perceptive Patient project where we investigated training medical doctors communication skills. Doctors’ communication styles are known to be predictors of patients’ satisfaction, as well as actual health outcomes. We are designing emotional state and behavioral expression models for virtual patients, and creating new architectures for online, distributed systems for human-virtual agent interaction. Our system demo at the 2018 International Meeting on Simulation in Healthcare received the “Best of Show” demo award.\n  ","date":1548284400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548284400,"objectID":"1c4338a7d761a226f9b14cf836a9f32b","permalink":"https://matchollet.github.io/project/perceptivepatient/","publishdate":"2019-01-24T00:00:00+01:00","relpermalink":"/project/perceptivepatient/","section":"project","summary":"Perceptive Virtual Patients for Improving Medical Communication","tags":["PerceptivePatient","Virtual Agents","Social Skills Training"],"title":"Perceptive Patient","type":"project"},{"authors":null,"categories":null,"content":"As part of my Master’s thesis, I proposed a model for automatic editing of 3D movies, that can learn from examples. Using a 3D virtual environment (CineSys) and a description of a scenario (i.e. timed actor actions), it casts the problem of film editing as a path-planning problem in a space of available cameras (pre-defined positions or automatically computed from actors positions). I defined cost functions for shot quality (e.g. shot type preferences, composition, visibility of actions), for cutting between shots (e.g. jump cuts, do not break the line of action), and for pacing. Each rule is associated to a weight: these weights can be automatically learned from manually produced movie examples, effectively representing the editing style of a particular user.\n  ","date":1548284400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548284400,"objectID":"1caa11d5d3dd5071b56bc4f836c98e32","permalink":"https://matchollet.github.io/project/cinesys/","publishdate":"2019-01-24T00:00:00+01:00","relpermalink":"/project/cinesys/","section":"project","summary":"Automatic Editing of 3D Movies","tags":["CineSys","Interactive Storytelling"],"title":"CineSys","type":"project"},{"authors":["Mathieu Chollet","Stacy Marsella","Stefan Scherer"],"categories":[],"content":"","date":1615023310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615023310,"objectID":"055f2de275ea37a084ea3203cb77e361","permalink":"https://matchollet.github.io/publication/jmuitraining/","publishdate":"2021-05-02T11:35:10+02:00","relpermalink":"/publication/jmuitraining/","section":"publication","summary":"Social signal processing and virtual social interaction technologies have allowed the creation of social skills training applications, and initial studies have shown that such solutions can lead to positive training outcomes and could complement traditional teaching methods by providing cheap, accessible, safe tools for training social skills. However, these studies evaluated social skills training systems as a whole and it is unclear to what extent which components contributed to positive outcomes. In this paper, we describe an experimental study where we compared the relative efficacy of real-time interactive feedback and after-action feedback in the context of a public speaking training application. We observed that both components provide benefits to the overall training: the real-time interactive feedback made the experience more immersive and improved participants’ motivation in using the system, while the after-action feedback led to positive training outcomes when it contained personalized feedback elements. Taken in combination, these results confirm that both social signal processing technologies and virtual social interactions are both contributing to social skills training systems’ efficiency. Additionally, we observed that several individual factors, here the subjects’ initial level of public speaking anxiety, personality and tendency to immersion significantly influenced the training experience. This finding suggests that social skills training systems could benefit from being tailored to participants’ particular individual circumstances. ","tags":[],"title":"Training public speaking with virtual social interactions: effectiveness of real-time feedback and delayed feedback","type":"publication"},{"authors":["Aurelien Lechappe","Mathieu Chollet","Jérôme Rigaud","Caroline GL Cao"],"categories":[],"content":"","date":1603618499,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603618499,"objectID":"22a7a9bbd40f8bf58af141b5cbda41b6","permalink":"https://matchollet.github.io/publication/msecp20sawareness/","publishdate":"2021-05-02T11:34:59+02:00","relpermalink":"/publication/msecp20sawareness/","section":"publication","summary":"The use of robotic surgical systems disrupts existing team dynamics inside operating rooms and constitutes a major challenge for the development of crucial non-technical skills such as situation awareness (SA). Techniques for assessing SA mostly rely on subjective assessments and questionnaires; few leverage multimodal measures combining physiological, behavioural, and subjective indicators. We propose a conceptual model relating SA with mental workload, stress and communication, supported by measurable behaviours and physiological signals. To validate this model, we collect subjective, behavioural, and physiological data from surgical teams performing radical prostatectomy using robotic surgical systems. Statistical analyses will be performed to establish relationships between SA, subjective assessment of stress and mental workload, communication processes, and the surgeons' physiological signals.","tags":[],"title":"Assessment of Situation Awareness during Robotic Surgery using Multimodal Data","type":"publication"},{"authors":["Enora Gabory","Mathieu Chollet"],"categories":[],"content":"","date":1603618499,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603618499,"objectID":"805bdf08b0e66f0f7ef972a1991593ff","permalink":"https://matchollet.github.io/publication/samih20sound/","publishdate":"2021-05-02T11:34:59+02:00","relpermalink":"/publication/samih20sound/","section":"publication","summary":"Virtual reality has demonstrated successful outcomes for treating social anxiety disorders, or helping to improve social skills. Some studies showed that various factors can impact the level of participants' anxiety during public speaking. However, the influence of sound design on this anxiety has been less investigated, and it is necessary to study the possible impacts that it can have. In this paper, we propose a model relating sound design concepts to presence and anxiety during virtual reality interactions, and present a protocol of a future experimental study aimed at investigating how sound design and in particular sound distractions can influence anxiety during public speaking simulations in virtual environments.","tags":[],"title":"Investigating the Influence of Sound Design for Inducing Anxiety in Virtual Public Speaking","type":"publication"},{"authors":["Mathieu Chollet","Talie Massachi","Stefan Scherer"],"categories":[],"content":"","date":1591176864,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591176864,"objectID":"d6331eb5cec10e39c78f02f5f5c140c2","permalink":"https://matchollet.github.io/publication/wacai20stress/","publishdate":"2021-05-02T11:34:24+02:00","relpermalink":"/publication/wacai20stress/","section":"publication","summary":"Des agents virtuels peuvent être utilisés pour recréer des situations de prise de parole en public simulées devant un public virtuel. En psychothérapie, de tels publics virtuels ont été utilisés pour lutter contre l’anxiété à la prise de parole en public, des études préliminaires montrant leurs bénéfices potentiels. Des travaux récents ont étudié comment les comportements de publics virtuels sont perçus par des utilisateurs. Cependant, nous ne savons toujours pas dans quelle mesure certains facteurs, tels que les comportements du public virtuel, ont un impact sur les utilisateurs lorsqu’ils interagissent dans ces simulations. Dans cet article, nous présentons une étude expérimentale pour étudier les états physiologiques d’utilisateurs ainsi que leurs auto-évaluations lors d’interactions avec un public virtuel. Nous observons que les comportements du public virtuel n’ont pas influencé les auto-évaluations des participants ni leurs réponses physiologiques, qui étaient ici plutôt déterminées en premier lieu par le niveau d’anxiété pré-éxistant des participants.","tags":[],"title":"Etude de l'influence du comportement d'un public virtuel sur le stress ressenti lors de prises de paroles simulées","type":"publication"},{"authors":["Nesrine Fourati","Mathieu Chollet"],"categories":[],"content":"","date":1583487273,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583487273,"objectID":"3951f205619894b580ddf76a9fb329a9","permalink":"https://matchollet.github.io/publication/wacai20feedback/","publishdate":"2021-05-02T11:34:33+02:00","relpermalink":"/publication/wacai20feedback/","section":"publication","summary":"Motivés par le besoin de former les collaborateurs d’entreprises du secteur industriel, nous présentons un système de production de feedbacks pour l’entraînement de la prise de parole en public. Dans un premier temps, la description de la performance est réduite à l’expression vocale.","tags":[],"title":"Vers un système de production de feedbacks personnalisé pour l'entraînement de la prise de parole en public","type":"publication"},{"authors":["Carolyn Saund","Marion Roth","Mathieu Chollet","Stacy Marsella"],"categories":[],"content":"","date":1567503238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567503238,"objectID":"6f5e5141e8ebcc2220986b8276d37102","permalink":"https://matchollet.github.io/publication/acii19gesture/","publishdate":"2021-05-02T11:33:58+02:00","relpermalink":"/publication/acii19gesture/","section":"publication","summary":"The use of metaphoric gestures by speakers has long been known to influence thought in the viewer. What is less clear is the extent to which the expression of multiple metaphors in a single gesture reliably affect viewer interpretation. Additionally, gestures which express only one metaphor are not sufficient to explain the broad array of metaphoric gestures and metaphoric scenes that human speakers naturally produce. In this paper we address three issues related to the implementation of metaphoric gestures in virtual humans. First, we break down naturally occurring examples of multiple-metaphor gestures, as well as metaphoric scenes created by gesture sequences. Then, we show the importance of capturing multiple metaphoric aspects of gesture with a behavioral experiment using crowdsourced judgements of videos of alterations of the naturally occurring gestures. Finally, we discuss the challenges for computationally modeling metaphoric gestures that are raised by our findings.","tags":[],"title":"Multiple metaphors in metaphoric gesturing","type":"publication"},{"authors":["J.-L. Lugrin","M. E. Latoschik","A.-G. Bosser","Y. Glemarec","B. Lugrin","M. Chollet"],"categories":null,"content":"","date":1549839600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549839600,"objectID":"a2ac19f303ad4d87bb9c233d3630f995","permalink":"https://matchollet.github.io/publication/chi19classroom/","publishdate":"2019-02-11T00:00:00+01:00","relpermalink":"/publication/chi19classroom/","section":"publication","summary":"We propose the integration of interactive narrative techniques for audience atmosphere generation in Virtual Reality (VR) training systems. Such a combination provides a number of advantages when compared with current atmosphere generation techniques as outlined in this paper. Features of recent Interactive Storytelling (IS) prototypes can be extended to automatically adapt the atmosphere produced by a group of virtual humans in response to user intervention, while still staying coherent with the unfolding training scenario story. This work is currently being developed in the context of a VR training for teachers, in which they learn to manage a difficult classroom under the guidance of an instructor. We believe our interactive, narrative-driven atmosphere generation approach and prototype will interest the CHI community working on autonomous virtual humans for training, educational or social applications","tags":["BBB","Virtual Audiences","Virtual Agents","Socia Skills Training"],"title":"Towards Narrative-Driven Atmosphere for Virtual Classroom","type":"publication"},{"authors":["Yann Glémarec","Anne-Gwenn Bosser","Cédric Buche","Jean-Luc Lugrin","Maximilian Landeck","Marc Erich Latoschik","Mathieu Chollet"],"categories":[],"content":"","date":1547285650,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547285650,"objectID":"cf128c4ff0aa5e2460e5a12a8d61cdbb","permalink":"https://matchollet.github.io/publication/vrst19scalability/","publishdate":"2021-05-02T11:34:10+02:00","relpermalink":"/publication/vrst19scalability/","section":"publication","summary":"In this paper, we describe the implementation and performance of a Virtual Audience perception model for Virtual Reality (VR). The model is a VR adaptation of an existing desktop model. The system allows a user in VR to easily build and experience a wide variety of atmospheres with small or large groups of virtual agents.The paper describes results of early evaluations for this model in VR. Our first scalability benchmark results demonstrated the ability to simultaneously handle one hundred virtual agents without significantly affecting there commended frame rate for VR applications.This research is conducted in the context of a classroom simulation software for teachers’ training.","tags":[],"title":"A Scalability Benchmark for a Virtual Audience Perception Model in Virtual Reality","type":"publication"},{"authors":null,"categories":null,"content":"+++ title = \u0026ldquo;Recent \u0026amp; Upcoming Talks\u0026rdquo; date = 2017-01-01T00:00:00\nList format. 0 = Simple 1 = Detailed 2 = Stream list_format = 2\nOptional featured image (relative to static/img/ folder). [header] image = \u0026quot;\u0026quot; caption = \u0026quot;\u0026quot; +++\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"0d23de87c496bda328fe2234e4594a01","permalink":"https://matchollet.github.io/talk/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talk/","section":"","summary":"Talks","tags":null,"title":"Talks","type":"widget_page"},{"authors":["M. Chollet","C. Neubauer","P. Ghate","S. Scherer"],"categories":null,"content":"","date":1541026800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541026800,"objectID":"2947270c257ddb289d6527a37849afcc","permalink":"https://matchollet.github.io/publication/iva18influence/","publishdate":"2018-11-01T00:00:00+01:00","relpermalink":"/publication/iva18influence/","section":"publication","summary":"Multimodal interaction technologies have enabled new applications for training interpersonal skills such as public speaking. Various training paradigms have been proposed, most of them relying on some form of graphical feedback provided to the trainee in real-time during their training or after training using an after-action review tool. Another paradigm consists of using virtual characters to provide feedback through their behavior during simulated social interactions. Preliminary studies have started to explore the effectiveness of these different training paradigms; however, these have not investigated the impact of individual differences on which interaction paradigm is more efficient or motivating for different populations of users. In this article, we explore the impact of personality, public speaking anxiety, and immersive tendencies on the experiences of users training public speaking with an interactive virtual audience system providing real-time feedback through virtual audience behavior as well as delayed feedback with an after-action review tool. We found that these three factors impacted different output measures of user experience and user ratings of the system's quality.","tags":["Cicero","Social Skills Training","Virtual Audiences"],"title":"Influence of individual differences when training public speaking with virtual audiences","type":"publication"},{"authors":["J. Wu","S. Ly","S. Ghosh","S. Mozgai","M. Chollet","S. Scherer"],"categories":null,"content":"","date":1541026800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541026800,"objectID":"9477b75bc0bd0eed7b4ce67859e1a961","permalink":"https://matchollet.github.io/publication/iva18nadia/","publishdate":"2018-11-01T00:00:00+01:00","relpermalink":"/publication/iva18nadia/","section":"publication","summary":"Advances in artificial intelligence and in particular machine learning and neural networks have given rise to a new generation of virtual assistants and chatbots. Within this work, we present NADiA - Neurally Animated Dialog Agent - that leverages both the user's verbal input as well as their facial expressions to respond in a meaningful way. NADiA combines a neural language model that generates appropriate responses to user prompts, a convolutional neural network for facial expression analysis, and virtual human technology that is deployed on a mobile phone. Here, we evaluate NADiA's anthropomorphic characteristics and its ability to understand the human interlocutor using both subjective as well as objective measures. We find that NADiA significantly outperforms state of the art chatbot technology and produces comparable behavior to human generated reference outputs.","tags":["NADiA","Virtual Agents","Machine Learning"],"title":"NADIA - Neural network driven virtual human conversation agents","type":"publication"},{"authors":["M. Chollet","P. Ghate","S. Scherer"],"categories":null,"content":"","date":1530396000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530396000,"objectID":"525d406642246a87b14c312745592e86","permalink":"https://matchollet.github.io/publication/aamas18generic/","publishdate":"2018-07-01T00:00:00+02:00","relpermalink":"/publication/aamas18generic/","section":"publication","summary":"We present a versatile system for training social skills with interactive virtual agents reacting to the user's automatically assessed performance.","tags":["Cicero","Social Skills Training","Virtual Agents","Multimodal Behavior Understanding"],"title":"A generic platform for training social skills with adaptative virtual agents","type":"publication"},{"authors":["J. Wu","S. Ly","S. Ghosh","S. Mozgai","M. Chollet","S. Scherer"],"categories":null,"content":"","date":1530396000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530396000,"objectID":"ca36f6c558f9be0bb699549ba45a3fea","permalink":"https://matchollet.github.io/publication/aamas18nadia/","publishdate":"2018-07-01T00:00:00+02:00","relpermalink":"/publication/aamas18nadia/","section":"publication","summary":"Advances in artificial intelligence and machine learning - in particular neural networks - have given rise to a new generation of virtual assistants and chatbots. Within this work, we describe the motivation and architecture of NADiA - Neurally Animated Dialog Agent - which leverages both the user's verbal input and facial expressions for multi-modal conversation. NADiA combines a neural language model that generates conversational responses, a convolutional neural network for facial expression analysis, and virtual human technology that is deployed on a mobile phone.","tags":["NADiA","Machine Learning","Virtual Agents"],"title":"NADiA - Towards neural network driven virtual human conversation agents","type":"publication"},{"authors":["C. Neubauer","M. Chollet","S. Mozgai","M. Dennison","P. Khooshabeh","S. Scherer"],"categories":null,"content":"","date":1509490800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509490800,"objectID":"b9817d738e45f22607655bf13f37782d","permalink":"https://matchollet.github.io/publication/icmi17physiology/","publishdate":"2017-11-01T00:00:00+01:00","relpermalink":"/publication/icmi17physiology/","section":"publication","summary":"It is commonly known that a relationship exists between the human voice and various emotional states. Past studies have demonstrated changes in a number of vocal features, such as fundamental frequency f0 and peakSlope, as a result of varying emotional state. These voice characteristics have been shown to relate to emotional load, vocal tension, and, in particular, stress. Although much research exists in the domain of voice analysis, few studies have assessed the relationship between stress and changes in the voice during a dyadic team interaction. The aim of the present study was to investigate the multimodal interplay between speech and physiology during a high-workload, high-stress team task. Specifically, we studied task-induced effects on participants' vocal signals, specifically, the f0 and peakSlope features, as well as participants' physiology, through cardiovascular measures. Further, we assessed the relationship between physiological states related to stress and changes in the speaker's voice. We recruited participants with the specific goal of working together to diffuse a simulated bomb. Half of our sample participated in an \"Ice Breaker\" scenario, during which they were allowed to converse and familiarize themselves with their teammate prior to the task, while the other half of the sample served as our \"Control\". Fundamental frequency (f0), peakSlope, physiological state, and subjective stress were measured during the task. Results indicated that f0 and peakSlope significantly increased from the beginning to the end of each task trial, and were highest in the last trial, which indicates an increase in emotional load and vocal tension. Finally, cardiovascular measures of stress indicated that the vocal and emotional load of speakers towards the end of the task mirrored a physiological state of psychological \"threat\".","tags":["Multimodal Behavior Understanding","Physiological Stress"],"title":"The relationship between task-induced stress, vocal changes, and physiological state during a dyadic team task","type":"publication"},{"authors":["M. Chollet","M. Ochs","C. Pelachaud"],"categories":null,"content":"","date":1504216800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504216800,"objectID":"37a8a9101870069e5fa3acb1b6c47d51","permalink":"https://matchollet.github.io/publication/taffc17methodology/","publishdate":"2017-09-01T00:00:00+02:00","relpermalink":"/publication/taffc17methodology/","section":"publication","summary":"IEEE In many applications, Embodied Conversational Agents (ECAs) must be able to express various affects such as emotions or social attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. Social attitudes affect the whole behavior of a person: they are \u0026#x201C;characteristic of an affective style that colors the entire interaction\u0026#x201D; (Scherer, 2005). Moreover, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with surrounding signals. Non-verbal behavior planning models designed to allow ECAs to express attitudes should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The contribution of this paper is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of a social phenomenon from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. This methodology is applied to design a virtual recruiter capable of expressing social attitudes, which is then evaluated in and out of an interaction context.","tags":["Tardis","Virtual Agents","Social Attitudes"],"title":"A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes","type":"publication"},{"authors":["M. Chollet","S. Scherer"],"categories":null,"content":"","date":1503266400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503266400,"objectID":"46453069c38d575bcff45905be6e468d","permalink":"https://matchollet.github.io/publication/cga17perception/","publishdate":"2017-08-21T00:00:00+02:00","relpermalink":"/publication/cga17perception/","section":"publication","summary":"A growing body of evidence shows that virtual audiences are a valuable tool in the treatment of social anxiety, and recent works show that it also a useful in public-speaking training programs. However, little research has focused on how such audiences are perceived and on how the behavior of virtual audiences can be manipulated to create various types of stimuli. The authors used a crowdsourcing methodology to create a virtual audience nonverbal behavior model and, with it, created a dataset of videos with virtual audiences containing varying behaviors. Using this dataset, they investigated how virtual audiences are perceived and which factors affect this perception.","tags":["Cicero","Virtual Audiences","Virtual Agents"],"title":"Perception of virtual audiences","type":"publication"},{"authors":["M. Chollet","T. Massachi","S. Scherer"],"categories":null,"content":"","date":1501538400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501538400,"objectID":"1c0f600d20b30825df4654dd1fbf3b23","permalink":"https://matchollet.github.io/publication/iva17racing/","publishdate":"2017-08-01T00:00:00+02:00","relpermalink":"/publication/iva17racing/","section":"publication","summary":"In psychotherapy, virtual audiences have been shown to promote successful outcomes when used to help treating public speaking anxiety. Additionally, early experiments have shown its potential to help improve public speaking ability. However, it is still unclear to what extent certain factors, such as audience non-verbal behaviors, impact users when interacting with a virtual audience. In this paper, we design an experimental study to investigate users' self-assessments and physiological states when interacting with a virtual audience. Our results showed that virtual audience behaviors did not influence participants self-assessments or physiological responses, which were instead predominantly determined by participants' prior anxiety levels.","tags":["Cicero","Physiological Stress","Social Skills Training","Virtual Audiences"],"title":"Racing heart and sweaty palms: What influences users' self-assessments and physiological signals when interacting with virtual audiences?","type":"publication"},{"authors":["S. Ghosh","M. Chollet","E. Laksana","L.-P. Morency","S. Scherer"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"e79d7a5ba22e501350f541be0903839a","permalink":"https://matchollet.github.io/publication/acl17affectlm/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/acl17affectlm/","section":"publication","summary":"Human verbal communication includes affective messages which are conveyed through use of emotionally colored words. There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration. In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories. Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter. Perception studies conducted using Amazon Mechanical Turk show that Affect-LM generates naturally looking emotional sentences without sacrificing grammatical correctness. Affect-LM also learns affect-discriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction.","tags":["Machine Learning"],"title":"Affect-LM: A neural language model for customizable affective text generation","type":"publication"},{"authors":["M. Chollet","S. Scherer"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"bf81a5820109c0f64e63ab36d680cd51","permalink":"https://matchollet.github.io/publication/fg17assessing/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/fg17assessing/","section":"publication","summary":"An important aspect of public speaking is delivery, which consists of the appropriate use of non-verbal cues to strengthen the message. Recent works have successfully predicted ratings of public speaking delivery aspects using the entire presentations of speakers. However, in other contexts, such as the assessment of personality or the prediction of job interview outcomes, it has been shown that thin slices, brief excerpts of behavior, provide enough information for raters to make accurate predictions. In this paper, we consider the use of thin slices for predicting ratings of public speaking behavior. We use a publicly available corpus of public speaking presentations and obtain ratings of full videos and thin slices. We first study how thin slices ratings are related to full video ratings. Then, we use automatic audio-visual feature extraction methods and machine learning algorithms to create models for predicting public speaking ratings, and evaluate these models for predicting thin slices ratings and full videos ratings.","tags":["Cicero","Multimodal Behavior Understanding","Machine Learning","Social Skills Training"],"title":"Assessing Public Speaking Ability from Thin Slices of Behavior","type":"publication"},{"authors":["B. Ravenet","F., Pecune","M., Chollet","C., Pelachaud"],"categories":null,"content":"","date":1477954800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477954800,"objectID":"dbf5c1418c2bb62122bb86e229055fe0","permalink":"https://matchollet.github.io/publication/eig16emotion/","publishdate":"2016-11-01T00:00:00+01:00","relpermalink":"/publication/eig16emotion/","section":"publication","summary":"Within this chapter, we are presenting how game developers could take inspiration from the research in Embodied Conversational Agent to develop non-player characters capable of expressing believable emotional and social reactions. Inspired  by  the  social  theories  about human emotional and social reactions,  the researchers  working  with  Embodied  Conversational  Agents  developed  different computational models to reproduce these human mechanisms within virtual characters. We are listing some of these works, comparing the different approaches and theories considered.","tags":["Emotions","Social Attitudes","Virtual Agents"],"title":"Emotion and Attitude Modeling for Non-player Characters","type":"publication"},{"authors":["M. Chollet","H. Prendinger","S. Scherer"],"categories":null,"content":"","date":1477954800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477954800,"objectID":"57791f56443de7dbb2325d45b9c065ed","permalink":"https://matchollet.github.io/publication/icmi16native/","publishdate":"2016-11-01T00:00:00+01:00","relpermalink":"/publication/icmi16native/","section":"publication","summary":" New technological developments in the field of multimodal interaction show great promise for the improvement and assessment of public speaking skills. However, it is unclear how the experience of non-native speakers interacting with such technologies differs from native speakers. In particular, nonnative speakers could benefit less from training with multimodal systems compared to native speakers. Additionally, machine learning models trained for the automatic assessment of public speaking ability on data of native speakers might not be performing well for assessing the performance of non-native speakers. In this paper, we investigate two aspects related to the performance and evaluation of multimodal interaction technologies designed for the improvement and assessment of public speaking between a population of English native speakers and a population of non-native English speakers. Firstly, we compare the experiences and training outcomes of these two populations interacting with a virtual audience system designed for training public speaking ability, collecting a dataset of public speaking presentations in the process. Secondly, using this dataset, we build regression models for predicting public speaking performance on both populations and evaluate these models, both on the population they were trained on and on how they generalize to the second population.","tags":["Cicero","Social Skills Training","Virtual Audiences","Multimodal Behavior Understanding"],"title":"Native vs. Non-native language fluency implications on multimodal interaction for interpersonal skills training","type":"publication"},{"authors":["U. Bernardet","M. Chollet","S. DiPaola","S. Scherer"],"categories":null,"content":"","date":1473026400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473026400,"objectID":"8efa766f0e97f3996f22bba6cc602e73","permalink":"https://matchollet.github.io/publication/iva16biologically/","publishdate":"2016-09-05T00:00:00+02:00","relpermalink":"/publication/iva16biologically/","section":"publication","summary":"In this paper, we present a reflexive behavior architecture, that is geared towards the application in the control of the non-verbal behavior of the virtual humans in a public speaking training system. The model is organized along the distinction between behavior triggers that are internal (endogenous) to the agent, and those that origin in the environment (exogenous). The endogenous subsystem controls gaze behavior, triggers self-adaptors, and shifts between different postures, while the exogenous system controls the reaction towards auditory stimuli with different temporal and valence characteristics. We evaluate the different components empirically by letting participants compare the output of the proposed system to valid alternative variations.","tags":["Virtual Agents"],"title":"An architecture for biologically grounded real-time reflexive behavior","type":"publication"},{"authors":["M. Chollet","N. Chandrashekhar","A. Shapiro","L.-P. Morency","S. Scherer"],"categories":null,"content":"","date":1473026400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473026400,"objectID":"38f7fdef5c63364ccb9eb234f7eaf23f","permalink":"https://matchollet.github.io/publication/iva16manipulating/","publishdate":"2016-09-05T00:00:00+02:00","relpermalink":"/publication/iva16manipulating/","section":"publication","summary":"Virtual audiences are used for training public speaking and mitigating anxiety related to it. However, research has been scarce on studying how virtual audiences are perceived and which non-verbal behaviors should be used to make such an audience appear in particular states, such as boredom or engagement. Recently, crowdsourcing methods have been proposed for collecting data for building virtual agents' behavior models. In this paper, we use crowdsourcing for creating and evaluating a nonverbal behaviors generation model for virtual audiences. We show that our model successfully expresses relevant audience states (i.e. low to high arousal, negative to positive valence), and that the overall impression exhibited by the virtual audience can be controlled my manipulating the amount of individual audience members that display a congruent state.","tags":["Cicero","Virtual Audiences","Virtual Agents"],"title":"Manipulating the perception of virtual audiences using crowdsourced behaviors","type":"publication"},{"authors":["M., Chollet","T., Massachi","S., Scherer"],"categories":null,"content":"","date":1472680800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472680800,"objectID":"817a27a822763bab59be8bbf9f999c06","permalink":"https://matchollet.github.io/publication/pava16stress/","publishdate":"2016-09-01T00:00:00+02:00","relpermalink":"/publication/pava16stress/","section":"publication","summary":"Virtual audiences have been used in psychotherapy for the treatment of public speaking anxiety, and recent studies show promising results with patients undergoing cognitive-behavior therapy with virtual reality exposure maintaining a reduction in their anxiety disorder for a year after treatment. It has been shown virtual exhibiting positive or negative behavior trigger different stress responses, however research on the topic of the effect of virtual audience behaviors has been scarce. In particular, it is unclear how variations in audience behavior can make the user’s stress levels vary while they are presenting. In this paper, we present a study where we intend to investigate the relationship between virtual audience behaviors and physiological measurements of stress. We use the Cicero virtual audience framework which allows for precise manipulation of its perceived level of arousal and valence by incremental changes in individual audience members behaviors. Additionally, we introduce a concept of a stress-aware virtual audience for public speaking training, which uses physiological assessments and virtual audience stimuli to maintain the user in a challenging, non-threatening state.","tags":["Cicero","Virtual Agents","Social Skills Training","Physiological Stress"],"title":"Investigating the Physiological Responses to Virtual Audience Behavioral Changes","type":"publication"},{"authors":["M. Chollet","T. Wörtwein","L.-P. Morency","S. Scherer"],"categories":null,"content":"","date":1462053600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462053600,"objectID":"bd55f470ef82b700745440116c545028","permalink":"https://matchollet.github.io/publication/lrec16corpus/","publishdate":"2016-05-01T00:00:00+02:00","relpermalink":"/publication/lrec16corpus/","section":"publication","summary":"The ability to efficiently speak in public is an essential asset for many professions and is used in everyday life. As such, tools enabling the improvement of public speaking performance and the assessment and mitigation of anxiety related to public speaking would be very useful. Multimodal interaction technologies, such as computer vision and embodied conversational agents, have recently been investigated for the training and assessment of interpersonal skills. Once central requirement for these technologies is multimodal corpora for training machine learning models. This paper addresses the need of these technologies by presenting and sharing a multimodal corpus of public speaking presentations. These presentations were collected in an experimental study investigating the potential of interactive virtual audiences for public speaking training. This corpus includes audio-visual data and automatically extracted features, measures of public speaking anxiety and personality, annotations of participants' behaviors and expert ratings of behavioral aspects and overall performance of the presenters. We hope this corpus will help other research teams in developing tools for supporting public speaking training.","tags":["Cicero","Multimodal Corpus","Social Skills Training"],"title":"A multimodal corpus for the assessment of public speaking ability and anxiety","type":"publication"},{"authors":["T. Wörtwein","L.-P. Morency","M. Chollet","R. Stiefelhagen","B. Schauerte","S. Scherer"],"categories":null,"content":"","date":1446332400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446332400,"objectID":"a9969f6511e8a50c4bf30b275232fbc5","permalink":"https://matchollet.github.io/publication/icmi15multimodal/","publishdate":"2015-11-01T00:00:00+01:00","relpermalink":"/publication/icmi15multimodal/","section":"publication","summary":"The ability to speak proficiently in public is essential for many professions and in everyday life. Public speaking skills are difficult to master and require extensive training. Recent developments in technology enable new approaches for public speaking training that allow users to practice in engaging and interactive environments. Here, we focus on the automatic assessment of nonverbal behavior and multimodal modeling of public speaking behavior. We automatically identify audiovisual nonverbal behaviors that are correlated to expert judges' opinions of key performance aspects. These automatic assessments enable a virtual audience to provide feedback that is essential for training during a public speaking performance. We utilize multimodal ensemble tree learners to automatically approximate expert judges' evaluations to provide post-hoc performance assessments to the speakers. Our automatic performance evaluation is highly correlated with the experts' opinions with r = 0.745 for the overall performance assessments. We compare multimodal approaches with single modalities and find that the multimodal ensembles consistently outperform single modalities.","tags":["Cicero","Multimodal Behavior Understanding","Machine Learning","Social Skills Training"],"title":"Multimodal public speaking performance assessment","type":"publication"},{"authors":["M. Chollet","K. Stefanov","H. Prendinger","S. Scherer"],"categories":null,"content":"","date":1446332400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446332400,"objectID":"3fb17781da36b145692671aaac96a2ab","permalink":"https://matchollet.github.io/publication/icmi15demo/","publishdate":"2015-11-01T00:00:00+01:00","relpermalink":"/publication/icmi15demo/","section":"publication","summary":"We have developed an interactive virtual audience platform for public speaking training. Users' public speaking behavior is automatically analyzed using multimodal sensors, and multimodal feedback is produced by virtual characters and generic visual widgets depending on the user's behavior. The flexibility of our system allows to compare different interaction mediums (e.g. virtual reality vs normal interaction), social situations (e.g. one-on-one meetings vs large audiences) and trained behaviors (e.g. general public speaking performance vs specific behaviors).","tags":["Cicero","Virtual Audiences","Multimodal Behavior Understanding","Social Skills Training"],"title":"Public speaking training with a multimodal interactive virtual audience framework","type":"publication"},{"authors":["M., Ochs","Y., Ding","N., Fourati","M., Chollet","B., Ravenet","F., Pecune","N., Glas","K., Prepin","C., Clavel","C., Pelachaud"],"categories":null,"content":"","date":1442872800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442872800,"objectID":"8092e1a1891c4d9efab4dd242732ffdc","permalink":"https://matchollet.github.io/publication/jips15agents/","publishdate":"2015-09-22T00:00:00+02:00","relpermalink":"/publication/jips15agents/","section":"publication","summary":"In this article, we propose an architecture of a socio-affective Embodied Conversational Agent (ECA). The different computational models of the architecture enable an ECA to express emotions and social attitudes during an interaction with a user. Based on corpora of actors expressing emotions, models have been defined to compute the emotional facial expressions of an ECA and the characteristics of its corporal movements. A user-perceptive approach has been used to design models to define how an ECA should adapt its non-verbal behavior according to the social attitude the ECA wants to display and the behavior of its interlocutor. The emotions and the social attitudes to express are computed by cognitive models presented in this article.","tags":["Virtual Agents","Emotions","Social Attitudes"],"title":"Vers des Agents Conversationnels Animés dotés d'émotions et d'attitudes sociales","type":"publication"},{"authors":["M. Chollet","T. Wörtwein","L.-P. Morency","A. Shapiro","S. Scherer"],"categories":null,"content":"","date":1441058400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441058400,"objectID":"0c98903ca5ccefadaf0fffe72ba17451","permalink":"https://matchollet.github.io/publication/ubicomp15exploring/","publishdate":"2015-09-01T00:00:00+02:00","relpermalink":"/publication/ubicomp15exploring/","section":"publication","summary":"Good public speaking skills convey strong and effective communication, which is critical in many professions and used in everyday life. The ability to speak publicly requires a lot of training and practice. Recent technological developments enable new approaches for public speaking training that allow users to practice in a safe and engaging environment. We explore feedback strategies for public speaking training that are based on an interactive virtual audience paradigm. We investigate three study conditions: (1) a non-interactive virtual audience (control condition), (2) direct visual feedback, and (3) nonverbal feedback from an interactive virtual audience. We perform a threefold evaluation based on self-Assessment questionnaires, expert assessments, and two objectively annotated measures of eye-contact and avoidance of pause fillers. Our experiments show that the interactive virtual audience brings together the best of both worlds: increased engagement and challenge as well as improved public speaking skills as judged by experts.","tags":["Cicero","Social Skills Training","Virtual Audiences"],"title":"Exploring feedback strategies to improve public speaking: An interactive virtual audience framework","type":"publication"},{"authors":["A.B. Youssef","M. Chollet","H. Jones","N. Sabouret","C. Pelachaud","M. Ochs"],"categories":null,"content":"","date":1438380000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1438380000,"objectID":"39f7899c1aa9842fde129a8884192362","permalink":"https://matchollet.github.io/publication/iva15adaptative/","publishdate":"2015-08-01T00:00:00+02:00","relpermalink":"/publication/iva15adaptative/","section":"publication","summary":"This paper presents a socially adaptive virtual agent that can adapt its behaviour according to social constructs (e.g. attitude, relationship) that are updated depending on the behaviour of its interlocutor. We consider the context of job interviews with the virtual agent playing the role of the recruiter. The evaluation of our approach is based on a comparison of the socially adaptive agent to a simple scripted agent and to an emotionally-reactive one. Videos of these three different agents in situation have been created and evaluated by 83 participants. This subjective evaluation shows that the simulation and expression of social attitude is perceived by the users and impacts on the evaluation of the agent's credibility. We also found that while the emotion expression of the virtual agent has an immediate impact on the user's experience, the impact of the virtual agent's attitude expression's impact is stronger after a few speaking turns.","tags":["Tardis","Social Attitudes","Virtual Agents"],"title":"Towards a socially adaptive virtual agent","type":"publication"},{"authors":["M., Chollet"],"categories":null,"content":"","date":1429567200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1429567200,"objectID":"cd5ce6db39f1aabb8d0b5cf424a60e79","permalink":"https://matchollet.github.io/publication/phd15/","publishdate":"2015-04-21T00:00:00+02:00","relpermalink":"/publication/phd15/","section":"publication","summary":"The Embodied Conversational Agents (ECAs) used in social training must be able to simulate all the different social situations that a learner has to train to. Depending on the application, the ECAs must then be able to express various emotions or various attitudes. Non-verbal signals, such as smiles or gestures, contribute to the expression of attitudes. However, recent findings have demonstrated that non-verbal signals are not interpreted in isolation but along with other signals : for instance, a smile followed by a gaze aversion and a head aversion does not signal amusement, but embarrassment. Non-verbal behavior planning models for ECAs should thus consider complete sequences of non-verbal signals and not only signals independently of one another. However, existing models do not take this into account, or in a limited manner. The main contribution of this thesis is a methodology for the automatic extraction of sequences of non-verbal signals characteristic of attitude variations from a multimodal corpus, and a non-verbal behavior planning model that takes into account sequences of non-verbal signals rather than signals independently. Another consideration in the design of social training systems is to check that users do improve their social skills while using such systems. We investigated the use of ECAs to build a virtual audience aimed at improving users’ public speaking skills. Another contribution of this thesis is the proposal of an architecture for interactive virtual audiences that provide real-time feedback to the learner according to his public speaking performance, and to have evaluated three different feedback strategies.","tags":["Virtual Agents"],"title":"Agents Conversationnels Animés pour l’entrainement social : modèle computationnel de l’expression d’attitudes sociales par des séquences de signaux non-verbaux","type":"publication"},{"authors":["A., Ben-Youssef","M., Chollet","H., Jones","N., Sabouret","C., Pelachaud","M., Ochs"],"categories":null,"content":"","date":1427583600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427583600,"objectID":"4d01a1a412800f14495dfb779a922fe4","permalink":"https://matchollet.github.io/publication/idgei15architecture/","publishdate":"2015-03-29T00:00:00+01:00","relpermalink":"/publication/idgei15architecture/","section":"publication","summary":"","tags":["Tardis","Virtual Agents","Social Attitudes","Social Skills Training"],"title":"An architecture for a socially adaptive virtual recruiter in job interview simulations","type":"publication"},{"authors":["M. Chollet","M. Ochs","C. Pelachaud"],"categories":null,"content":"","date":1406844000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406844000,"objectID":"a77d06fda6ec7861b368dd112e9ab94f","permalink":"https://matchollet.github.io/publication/iva14sequence/","publishdate":"2014-08-01T00:00:00+02:00","relpermalink":"/publication/iva14sequence/","section":"publication","summary":"In this paper, we present a model and its evaluation for expressing attitudes through sequences of non-verbal signals for Embodied Conversational Agents. To build our model, a corpus of job interviews has been annotated at two levels: the non-verbal behavior of the recruiters as well as their expressed attitudes was annotated. Using a sequence mining method, sequences of non-verbal signals characterizing different interpersonal attitudes were automatically extracted from the corpus. From this data, a probabilistic graphical model was built. The probabilistic model is used to select the most appropriate sequences of non-verbal signals that an ECA should display to convey a particular attitude. The results of a perceptive evaluation of sequences generated by the model show that such a model can be used to express interpersonal attitudes.","tags":["Tardis","Social Attitudes","Virtual Agents"],"title":"From non-verbal signals sequence mining to bayesian networks for interpersonal attitudes expression","type":"publication"},{"authors":["F., Pecune","A., Cafaro","M., Chollet","P., Philippe","C., Pelachaud"],"categories":null,"content":"","date":1406844000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406844000,"objectID":"464189b9296e91ac85e513aa2c6411ce","permalink":"https://matchollet.github.io/publication/wasiva14vib/","publishdate":"2014-08-01T00:00:00+02:00","relpermalink":"/publication/wasiva14vib/","section":"publication","summary":"","tags":["Virtual Agents"],"title":"Suggestions for extending saiba with the vib platform","type":"publication"},{"authors":["H., Jones","M., Chollet","M., Ochs","N., Sabouret","C., Pelachaud"],"categories":null,"content":"","date":1401573600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401573600,"objectID":"8318ce6c33f337703b514004f924959b","permalink":"https://matchollet.github.io/publication/wacai14expressing/","publishdate":"2014-06-01T00:00:00+02:00","relpermalink":"/publication/wacai14expressing/","section":"publication","summary":"The use of virtual agents in social coaching has increased rapidly in the last decade. In social coaching, the virtual agent should be able to express different social attitudes to train the user in different situations than can occur in real life. In this paper, we propose a model of social attitudes that enables a virtual agent to reason on the appropriate social attitude to express during the interaction with a user given the course of the interaction, but also the emotions, mood and personality of the agent. Moreover, the model enables the virtual agent to display its social attitude through its non-verbal behaviour.","tags":["Tardis","Social Attitudes","Virtual Agents","Social Skills Training"],"title":"Expressing social attitudes in virtual agents for social coaching","type":"publication"},{"authors":["M. Chollet","G. Sratou","A. Shapiro","L.-P. Morency","S. Scherer"],"categories":null,"content":"","date":1398895200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398895200,"objectID":"6fd75a09aac7afc9d37bb0780706a131","permalink":"https://matchollet.github.io/publication/aamas14audience/","publishdate":"2014-05-01T00:00:00+02:00","relpermalink":"/publication/aamas14audience/","section":"publication","summary":"We have developed an interactive virtual audience platform for public speaking training. Users' public speaking behavior is automatically analyzed using audiovisual sensors. The virtual characters display indirect feedback depending on user's behavior descriptors correlated with public speaking performance. We used the system to collect a dataset of public speaking performances in different training conditions.","tags":["Cicero","Multimodal Behavior Understanding","Social Skills Training","Virtual Audiences"],"title":"An interactive virtual audience platform for public speaking training","type":"publication"},{"authors":["H. Jones","M. Chollet","M. Ochs","N. Sabouret","C. Pelachaud"],"categories":null,"content":"","date":1398895200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398895200,"objectID":"795c09f41db35d89803eec53dd37f5b8","permalink":"https://matchollet.github.io/publication/aamas14expressing/","publishdate":"2014-05-01T00:00:00+02:00","relpermalink":"/publication/aamas14expressing/","section":"publication","summary":"This paper presents a model of social attitudes for reasoning about the appropriate attitude to express during an interaction. It combines a theoretical approach with a study of a corpus of human-to-human interactions.","tags":["Tardis","Social Attitudes","Social Skills Training","Virtual Agents"],"title":"Expressing social attitudes in virtual agents for social coaching","type":"publication"},{"authors":["M. Chollet","M. Ochs","C. Pelachaud"],"categories":null,"content":"","date":1398895200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398895200,"objectID":"80c5c81e128cb65cf66ac5475e3aa245","permalink":"https://matchollet.github.io/publication/lrec14mining/","publishdate":"2014-05-01T00:00:00+02:00","relpermalink":"/publication/lrec14mining/","section":"publication","summary":"Interpersonal attitudes are expressed by non-verbal behaviors on a variety of different modalities. The perception of these behaviors is influenced by how they are sequenced with other behaviors from the same person and behaviors from other interactants. In this paper, we present a method for extracting and generating sequences of non-verbal signals expressing interpersonal attitudes. These sequences are used as part of a framework for non-verbal expression with Embodied Conversational Agents that considers different features of non-verbal behavior: global behavior tendencies, interpersonal reactions, sequencing of non-verbal signals, and communicative intentions. Our method uses a sequence mining technique on an annotated multimodal corpus to extract sequences characteristic of different attitudes. New sequences of non-verbal signals are generated using a probabilistic model, and evaluated using the previously mined sequences.","tags":["Tardis","Social Attitudes","Virtual Agents"],"title":"Mining a multimodal corpus for non-verbal signals sequences conveying attitudes","type":"publication"},{"authors":["N., Sabouret","H. Jones","M., Ochs","M., Chollet","C., Pelachaud"],"categories":null,"content":"","date":1393196400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393196400,"objectID":"d1aa908797fe40418f0d3136afe45042","permalink":"https://matchollet.github.io/publication/idgei14expressing/","publishdate":"2014-02-24T00:00:00+01:00","relpermalink":"/publication/idgei14expressing/","section":"publication","summary":"The use of virtual agents in social coaching has increased rapidly in the last decade. In order to train the user in different situations than can occur in real life, the virtual agent should be able to express different social attitudes. In this paper, we propose a model of social attitudes that enables a virtual agent to reason on the appropriate social attitude to express during the interaction with a user given the course of the interaction, but also the emotions, mood and personality of the agent. Moreover, the model enables the virtual agent to display its social attitude through its non-verbal behaviour. The proposed model has been developed in the context of job interview simulation. The methodology used to develop such a model combined a theoretical and an empirical approach. Indeed, the model is based both on the literature in Human and Social Sciences on social attitudes but also on the analysis of an audiovisual corpus of job interviews and on post-hoc interviews with the recruiters on their expressed attitudes during the job interview.","tags":["Tardis","Virtual Agents","Social Attitudes","Social Skills Training"],"title":"Expressing social attitudes in virtual agents for social training games","type":"publication"},{"authors":["M., Chollet","M., Ochs","C., Pelachaud"],"categories":null,"content":"","date":1377986400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377986400,"objectID":"e0fd62e2efa6e39630ce33f010ccfa87","permalink":"https://matchollet.github.io/publication/mmsym13investigating/","publishdate":"2013-09-01T00:00:00+02:00","relpermalink":"/publication/mmsym13investigating/","section":"publication","summary":"The study of the expression of affects and their expression by Embodied Conversational Agents is complex. This is because affects are expressed by non-verbal behaviors on a variety of different modalities, and that these behaviors are influenced by the context of the interaction and other interactants’ behaviors. To overcome these challenges, we present a multi-layer framework and apply it to the study interpersonal stance dynamics. For this purpose, we built a corpus of non-verbal behavior annotations and interpersonal stance traces for job interviews.","tags":["Tardis","Social Attitudes"],"title":"Investigating Non-Verbal Behaviors Conveying Interpersonal Stances","type":"publication"},{"authors":["M., Chollet","M., Ochs","C., Pelachaud"],"categories":null,"content":"","date":1375308000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375308000,"objectID":"7e8c12fd11f06162d84e354aa1248cae","permalink":"https://matchollet.github.io/publication/mmc13corpus/","publishdate":"2013-08-01T00:00:00+02:00","relpermalink":"/publication/mmc13corpus/","section":"publication","summary":"","tags":["Tardis","Multimodal Corpus","Social Attitudes"],"title":"A multimodal corpus for the study of non-verbal behavior expressing interpersonal stances","type":"publication"},{"authors":["M. Chollet","M. Ochs","C. Clavel","C. Pelachaud"],"categories":null,"content":"","date":1356994800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356994800,"objectID":"acc9f36f975a5b58778dff62fddd022b","permalink":"https://matchollet.github.io/publication/acii13multimodal/","publishdate":"2013-01-01T00:00:00+01:00","relpermalink":"/publication/acii13multimodal/","section":"publication","summary":"This paper presents the analysis of the multimodal behavior of experienced practitioners of job interview coaching, and describes a methodology to specify their behavior in Embodied Conversational Agents acting as virtual recruiters displaying different interpersonal stances. In a first stage, we collect a corpus of videos of job interview enactments, and we detail the coding scheme used to encode multimodal behaviors and contextual information. From the annotations of the practitioners' behaviors we observe specificities of behavior across different levels, namely monomodal behavior variations, inter-modalities behavior influences, and contextual influences on behavior. Finally we propose the adaptation of an existing agent architecture to model these specificities in a virtual recruiter's behavior.","tags":["Tardis","Virtual Agents","Multimodal Corpus"],"title":"A multimodal corpus approach to the design of virtual recruiters","type":"publication"},{"authors":["K. Anderson","E. André","T. Baur","S. Bernardini","M. Chollet","E. Chryssafidou","I. Damian","C. Ennis","A. Egges","P. Gebhard","H. Jones","M. Ochs","C. Pelachaud","K. Porayska-Pomsta","P. Rizzo","N. Sabouret"],"categories":null,"content":"","date":1356994800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356994800,"objectID":"3795f2c109344f6afbd5f6b653aa6a31","permalink":"https://matchollet.github.io/publication/ace13tardis/","publishdate":"2013-01-01T00:00:00+01:00","relpermalink":"/publication/ace13tardis/","section":"publication","summary":"The TARDIS project aims to build a scenario-based serious-game simulation platform for NEETs and job-inclusion associations that supports social training and coaching in the context of job interviews. This paper presents the general architecture of the TARDIS job interview simulator, and the serious game paradigm that we are developing.","tags":["Tardis","Social Skills Training","Virtual Agents"],"title":"The TARDIS framework: Intelligent virtual agents for social coaching in job interviews","type":"publication"},{"authors":["M. Ochs","Y. Ding","N. Fourati","M. Chollet","B. Ravenet","F. Pecune","N. Glas","K. Prepin","C. Clavel","C. Pelachaud"],"categories":null,"content":"","date":1356994800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356994800,"objectID":"198fe2148aaa52c5db81ad609743160e","permalink":"https://matchollet.github.io/publication/ihm13agents/","publishdate":"2013-01-01T00:00:00+01:00","relpermalink":"/publication/ihm13agents/","section":"publication","summary":"Dans  cet  article,  nous  proposons  une  architecture  d'un Agent Conversationnel Animé (ACA) socio-affectif. Les différents  modèles  computationnels  sous-jacents  à  cette architecture, permettant de donner la capacité à  un  ACA d’exprimer  des  émotions  et  des  attitudes  sociales  durant son interaction avec l’utilisateur, sont présentés.","tags":["Virtual Agents","Emotions","Social Attitudes"],"title":"Vers des agents conversationnels animés socio-affectifs","type":"publication"},{"authors":["M., Chollet","M., Ochs","C., Pelachaud"],"categories":null,"content":"","date":1351724400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351724400,"objectID":"70a101603705f444ab013db7311771b8","permalink":"https://matchollet.github.io/publication/wacai12interpersonal/","publishdate":"2012-11-01T00:00:00+01:00","relpermalink":"/publication/wacai12interpersonal/","section":"publication","summary":"We present a computational model for interpreting nonverbal signals of a user during an interaction with a virtual character in order to obtain a representation of his interpersonal stance. Our model starts, on the one hand, from the analysis of multimodal signals. On the other hand, it takes into account the temporal patterns of the interactants behaviors. That is it analyses signals and reactions to signals in their immediate context, as well as features of signal production patterns and reaction patterns on different time windows : signal reaction, sentence reaction, conversation topic, whole interaction. In this paper, we propose a first model parameterized using data obtained from the literature on the expressions of stances through interpersonal behavior.","tags":["Social Attitudes","Multimodal Behavior Understanding"],"title":"Interpersonal stance recognition using non-verbal signals on several time windows","type":"publication"},{"authors":["C. Lino","M. Chollet","M. Christie","R. Ronfard"],"categories":null,"content":"","date":1320102000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1320102000,"objectID":"b9e110d63d231fcc302e32f7780d664d","permalink":"https://matchollet.github.io/publication/icids11computational/","publishdate":"2011-11-01T00:00:00+01:00","relpermalink":"/publication/icids11computational/","section":"publication","summary":"Generating interactive narratives as movies requires knowledge in cinematography (camera placement, framing, lighting) and film editing (cutting between cameras). We present a framework for generating a well-edited movie from interactively generated scene contents and cameras. Our system computes a sequence of shots by simultaneously choosing which camera to use, when to cut in and out of the shot, and which camera to cut to.","tags":["CineSys","Interactive Storytelling"],"title":"Computational model of film editing for interactive storytelling","type":"publication"},{"authors":["M., Chollet"],"categories":null,"content":"","date":1306879200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1306879200,"objectID":"0a25e3984adacbf82dc96fe0391cf90e","permalink":"https://matchollet.github.io/publication/master11/","publishdate":"2011-06-01T00:00:00+02:00","relpermalink":"/publication/master11/","section":"publication","summary":"The production of movies requires knowledge in cinematography, in particular on techniques such as framing, camera placement and editing. For a range of 3D applications such as film prototyping and interactive narration systems, the automatization of the editing process appears a useful way of providing shot sequences that convey the story and are correct with respect to cinematographic conventions. Existing systems have used limited amounts of carefully chosen cameras. This ensures that the edit respects cinematographic conventions, but lacks variability in directorial style. In this report, we introduce a model of cinematographic editing evaluating shots and cuts in a generic fashion. This casts the problem as independent from camera generation and allows the use of all kinds of shots and cuts. Furthermore, we propose the use of maching learning methods for tuning the parameters of the model with examples.","tags":["CineSys","Interactive Storytelling"],"title":"A virtual cinematography system that learns from examples","type":"publication"},{"authors":["C., Lino","M., Chollet","M., Christie","R., Ronfard"],"categories":null,"content":"","date":1304200800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1304200800,"objectID":"acd2d8c2d81968de66e173249f25823a","permalink":"https://matchollet.github.io/publication/sca11planner/","publishdate":"2011-05-01T00:00:00+02:00","relpermalink":"/publication/sca11planner/","section":"publication","summary":"Generating films from 3D animations requires knowledge in cinematography (camera placement, framing and lighting) and editing (cutting between cameras). In applications where the user is already engaged in other tasks, such as playing a game, directing virtual actors, or narrating a story, it appears desirable to build systems that can make decisions about cinematography and editing and automatically generate a grammatically correct movie according to film grammar. In this paper, we introduce a framework for generating a well-edited movie based on the rules of film editing. Specifically, our system computes a sequence of shots by simultaneously choosing which camera to use, when to cut in and out of the shot, and where to cut to. We cast film editing as a cost minimization problem in the space of possible shot sequences and provide an efficient search algorithm. The method is illustrated with multiple edits of the same footage in different editing rhythms and styles.","tags":["CineSys","Interactive Storytelling"],"title":"Automated Camera Planner for Film Editing Using Key Shots","type":"publication"}]